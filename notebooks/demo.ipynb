{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc07c2e2",
   "metadata": {},
   "source": [
    "# Ready Tensor RAG Assistant - Demo Notebook\n",
    "\n",
    "This notebook demonstrates the full workflow of the RAG-based assistant system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252f518d",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's install and import all necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289707d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai faiss-cpu python-dotenv\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56190a54",
   "metadata": {},
   "source": [
    "## 2. Initialize Components\n",
    "\n",
    "Set up the embeddings model and LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ae1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings and LLM\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "llm = ChatOpenAI(openai_api_key=api_key, temperature=0.7, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "print(\"âœ“ Embeddings and LLM initialized!\")\n",
    "print(f\"Embedding Model: OpenAI Embeddings\")\n",
    "print(f\"LLM Model: GPT-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e99bdb",
   "metadata": {},
   "source": [
    "## 3. Load and Process Documents\n",
    "\n",
    "Load sample documents and create embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8676305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents (replace with your actual documents)\n",
    "sample_documents = [\n",
    "    \"Ready Tensor is a platform for AI learning and certification.\",\n",
    "    \"RAG (Retrieval-Augmented Generation) combines document retrieval with language models.\",\n",
    "    \"Vector databases enable semantic search and similarity matching.\",\n",
    "    \"LangChain provides tools for building LLM applications.\",\n",
    "]\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    separator=\".\"\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "from langchain.schema import Document\n",
    "docs = [Document(page_content=doc) for doc in sample_documents]\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"âœ“ Loaded {len(sample_documents)} documents\")\n",
    "print(f\"âœ“ Split into {len(split_docs)} chunks\")\n",
    "print(\"\\nSample chunks:\")\n",
    "for i, doc in enumerate(split_docs[:3]):\n",
    "    print(f\"  {i+1}. {doc.page_content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11482f7",
   "metadata": {},
   "source": [
    "## 4. Create Vector Store\n",
    "\n",
    "Build and store embeddings in FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5df2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "print(f\"âœ“ Vector store created\")\n",
    "print(f\"âœ“ Total documents: {vectorstore.index.ntotal}\")\n",
    "print(\"âœ“ Vector store ready for retrieval!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b465499",
   "metadata": {},
   "source": [
    "## 5. Create QA Chain\n",
    "\n",
    "Set up the retrieval-based QA system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade6937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"âœ“ QA chain created!\")\n",
    "print(\"Ready for queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c534333",
   "metadata": {},
   "source": [
    "## 6. Test Queries\n",
    "\n",
    "Run sample queries against the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What is Ready Tensor?\",\n",
    "    \"How does RAG work?\",\n",
    "    \"What are vector databases used for?\",\n",
    "]\n",
    "\n",
    "print(\"Running test queries...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQ: {query}\")\n",
    "    print(\"-\" * 40)\n",
    "    result = qa_chain({\"query\": query})\n",
    "    print(f\"A: {result['result']}\")\n",
    "    print(f\"\\nSources used:\")\n",
    "    for doc in result.get('source_documents', []):\n",
    "        print(f\"  - {doc.page_content[:60]}...\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f9b3f4",
   "metadata": {},
   "source": [
    "## 7. Performance Metrics\n",
    "\n",
    "Analyze the performance of the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbec755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Performance Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Measure query latency\n",
    "latencies = []\n",
    "for query in test_queries:\n",
    "    start = time.time()\n",
    "    _ = qa_chain({\"query\": query})\n",
    "    latency = time.time() - start\n",
    "    latencies.append(latency)\n",
    "\n",
    "print(f\"Average latency: {sum(latencies)/len(latencies):.2f}s\")\n",
    "print(f\"Min latency: {min(latencies):.2f}s\")\n",
    "print(f\"Max latency: {max(latencies):.2f}s\")\n",
    "print(f\"\\nVector store info:\")\n",
    "print(f\"  - Documents indexed: {vectorstore.index.ntotal}\")\n",
    "print(f\"  - Embedding dimension: {vectorstore.index.d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd21c8",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "To extend this demo:\n",
    "\n",
    "1. **Load Real Documents**: Replace sample_documents with actual PDFs or text files\n",
    "2. **Add Conversation Memory**: Implement multi-turn conversations\n",
    "3. **Optimize Retrieval**: Tune chunk size, overlap, and search parameters\n",
    "4. **Deploy**: Convert to REST API or Streamlit web app\n",
    "5. **Monitor**: Add logging and observability\n",
    "\n",
    "Happy building! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
