# Quick Reference Guide - Ready Tensor RAG

Fast lookup for commands, code snippets, and configurations.

---

## ğŸš€ Installation (30 seconds)

```bash
# Clone
git clone https://github.com/ArnabSen08/ready-tensor.git
cd ready-tensor

# Setup
python -m venv venv
venv\Scripts\activate  # Windows: venv\Scripts\activate

# Install & Configure
pip install -r requirements.txt
echo OPENAI_API_KEY=sk-your-key > .env

# Run Demo
python demo.py
```

---

## ğŸ“ Common Commands

### Start Project
```bash
# Quick demo (no API key needed)
python demo.py

# Run interactive notebook
jupyter notebook notebooks/demo.ipynb

# Run main application
python src/rag_assistant.py
```

### Testing
```bash
# Run tests
pytest tests/

# Run benchmarks
pytest tests/benchmarks/ --benchmark

# Check syntax
python -m py_compile src/rag_assistant.py
```

### Git Workflow
```bash
# View changes
git status
git diff

# Commit changes
git add .
git commit -m "message"
git push origin master

# View history
git log --oneline -10
```

---

## ğŸ”§ Configuration Reference

### Environment Variables (.env)
```env
# Required
OPENAI_API_KEY=sk-...

# Optional
OPENAI_MODEL=gpt-3.5-turbo
EMBEDDING_MODEL=text-embedding-3-small
VECTORSTORE_PATH=./data/vectorstore
DOCUMENTS_PATH=./data/documents
LOG_LEVEL=INFO
```

### Python Dependencies
```bash
# Install specific version
pip install langchain==0.1.0

# Upgrade all
pip install -r requirements.txt --upgrade

# Add new dependency
pip install package-name
pip freeze > requirements.txt
```

### RAG Configuration
```python
# Chunking
chunk_size=1000
chunk_overlap=200

# Retrieval
retrieval_k=3
search_type="similarity"  # or "mmr"

# LLM
temperature=0.7
max_tokens=500

# Embeddings
embedding_model="text-embedding-3-small"
```

---

## ğŸ’» Code Snippets

### Basic RAG Query
```python
from ready_tensor import RAGAssistant

# Initialize
assistant = RAGAssistant(api_key="sk-...")

# Query
answer = assistant.query("What is RAG?")
print(answer)
```

### Build Vector Store
```python
from langchain.vectorstores import FAISS
from langchain.embeddings.openai import OpenAIEmbeddings

documents = load_documents("data/documents/")
chunks = split_documents(documents)

embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(chunks, embeddings)
vectorstore.save_local("data/vectorstore")
```

### Create QA Chain
```python
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever
)

answer = qa.run("Your question here")
```

### Load Different LLMs
```python
# OpenAI
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model="gpt-3.5-turbo")

# Ollama (local)
from langchain.llms import Ollama
llm = Ollama(model="llama2")

# Anthropic
from langchain.chat_models import ChatAnthropic
llm = ChatAnthropic(model="claude-instant")
```

### Streaming Responses
```python
# Stream tokens
for token in llm.stream(prompt):
    print(token, end='', flush=True)

# Streaming chain
response = qa.stream("Question")
for chunk in response:
    print(chunk['result'])
```

### Add Caching
```python
from langchain.cache import InMemoryCache
import langchain
langchain.llm_cache = InMemoryCache()

# Now LLM responses are cached
```

---

## ğŸ“Š API Reference

### RAGAssistant Class
```python
class RAGAssistant:
    def __init__(self, api_key, vectorstore_path)
    def load_vectorstore()
    def create_qa_chain()
    def query(question: str) -> str
    def query_with_sources(question: str) -> dict
```

### Configuration Class
```python
@dataclass
class RAGConfig:
    embedding_model: str
    chunk_size: int
    chunk_overlap: int
    retrieval_k: int
    temperature: float
    max_tokens: int
```

### Vector Store Options
```python
# FAISS (default)
FAISS.from_documents(docs, embeddings)

# Chroma
Chroma.from_documents(docs, embeddings)

# Pinecone
Pinecone.from_documents(docs, embeddings, index_name="index")
```

---

## ğŸ§ª Testing Snippets

### Test Vector Store
```python
def test_vectorstore():
    vectorstore = load_vectorstore()
    results = vectorstore.similarity_search("test query", k=3)
    assert len(results) == 3
```

### Test RAG Pipeline
```python
def test_rag_pipeline():
    assistant = RAGAssistant(api_key="test-key")
    answer = assistant.query("What is RAG?")
    assert answer is not None
    assert len(answer) > 0
```

### Mock LLM for Testing
```python
from langchain.llms.fake import FakeListLLM

responses = ["Answer 1", "Answer 2"]
llm = FakeListLLM(responses=responses)
```

---

## ğŸ“ˆ Performance Tuning

### Optimize Retrieval
```python
# Use MMR instead of similarity
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 3, "lambda_mult": 0.25}
)

# Reduce chunk size for faster retrieval
chunk_size = 500
```

### Optimize LLM
```python
# Use faster model
llm = ChatOpenAI(model="gpt-3.5-turbo")

# Reduce token generation
max_tokens = 256

# Temperature for consistency
temperature = 0.3
```

### Caching Strategy
```python
# Cache embeddings
from langchain.cache import RedisCache

langchain.llm_cache = RedisCache(redis_="redis://localhost:6379")
```

---

## ğŸ› Debugging

### Enable Verbose Logging
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Or in chain
qa = RetrievalQA.from_chain_type(
    llm, retriever,
    verbose=True,
    return_source_documents=True
)
```

### Inspect Retrieved Documents
```python
results = retriever.get_relevant_documents("query")
for doc in results:
    print(f"Score: {doc.metadata.get('score')}")
    print(f"Content: {doc.page_content[:100]}...")
```

### Test Embeddings
```python
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
test_embedding = embeddings.embed_query("test")
print(f"Embedding dimension: {len(test_embedding)}")
```

---

## ğŸ“š Documentation Links

| Resource | Link |
|----------|------|
| Main Publication | [PUBLICATION.md](../PUBLICATION.md) |
| Setup Guide | [setup.md](./setup.md) |
| Architecture | [architecture.md](./architecture.md) |
| Examples | [examples.md](./examples.md) |
| Benchmarks | [BENCHMARKS.md](./BENCHMARKS.md) |
| FAQ | [FAQ.md](./FAQ.md) |
| Deployment | [DEPLOYMENT.md](./DEPLOYMENT.md) |

---

## ğŸ”— Useful Links

```
GitHub: https://github.com/ArnabSen08/ready-tensor
Pages: https://arnabsen08.github.io/ready-tensor/
LangChain: https://python.langchain.com/
OpenAI API: https://platform.openai.com/docs/
FAISS: https://faiss.ai/
```

---

## âš¡ Performance Benchmarks

Quick reference for expected performance:

```
Query Latency:     1.2 seconds (end-to-end)
Retrieval Time:    50 milliseconds
LLM Inference:     1.1 seconds
Accuracy:          89%
Success Rate:      94%
Cost per Query:    $0.0005
Throughput:        3 queries/second
```

---

## ğŸ†˜ Troubleshooting Quick Fixes

| Issue | Quick Fix |
|-------|-----------|
| API key error | Check `.env` file and `OPENAI_API_KEY` |
| Module not found | Run `pip install -r requirements.txt` |
| Slow retrieval | Reduce `chunk_size` or use smaller embeddings |
| High latency | Check LLM model, reduce `max_tokens` |
| Out of memory | Limit document count or use cloud vector DB |
| Rate limiting | Add retry logic with exponential backoff |

---

## ğŸ“ File Locations

```
ready-tensor/
â”œâ”€â”€ src/rag_assistant.py          â† Main code
â”œâ”€â”€ demo.py                        â† Quick demo
â”œâ”€â”€ requirements.txt               â† Dependencies
â”œâ”€â”€ .env.example                   â† Config template
â”œâ”€â”€ notebooks/demo.ipynb           â† Interactive notebook
â”œâ”€â”€ data/documents/                â† Your documents
â”œâ”€â”€ data/vectorstore/              â† Generated index
â””â”€â”€ docs/
    â”œâ”€â”€ QUICK_REFERENCE.md         â† This file
    â”œâ”€â”€ setup.md
    â”œâ”€â”€ architecture.md
    â”œâ”€â”€ FAQ.md
    â””â”€â”€ BENCHMARKS.md
```

---

## ğŸ¯ Common Workflows

### Workflow 1: Quick Start
```bash
python demo.py           # See it work
jupyter notebook         # Learn interactively
python src/rag_assistant.py  # Run full system
```

### Workflow 2: Add Your Documents
```bash
# 1. Place documents in data/documents/
# 2. Run: python src/document_loader.py
# 3. Query: python src/rag_assistant.py
```

### Workflow 3: Deploy to Production
```bash
# 1. Set up virtual environment
# 2. Configure .env with secrets
# 3. Run: python -m gunicorn app:app
# 4. Or: docker-compose up
```

---

**Last Updated:** January 2026  
**Version:** 1.0.0
